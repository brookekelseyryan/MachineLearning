{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 4: Naïve Bayes Classifiers\n",
    "\n",
    "\n",
    "| x1           | x2       | x3             | x4          | x5            | y        |\n",
    "| ------------ | -------- | -------------- | ----------- | ------------- | -------- |\n",
    "| know author? | is long? | has 'research' | has 'grade' | has 'lottery' | => read? |\n",
    "| 0            | 0        | 1              | 1           | 0             | -1       |\n",
    "| 1            | 1        | 0              | 1           | 0             | -1       |\n",
    "| 0            | 1        | 1              | 1           | 1             | -1       |\n",
    "| 1            | 1        | 1              | 1           | 0             | -1       |\n",
    "| 0            | 1        | 0              | 0           | 0             | -1       |\n",
    "| 1            | 0        | 1              | 1           | 1             | 1        |\n",
    "| 0            | 0        | 1              | 0           | 0             | 1        |\n",
    "| 1            | 0        | 0              | 0           | 0             | 1        |\n",
    "| 1            | 0        | 1              | 1           | 0             | 1        |\n",
    "| 1            | 1        | 1              | 1           | 1             | -1       |\n",
    "\n",
    "## 4) A possible alternative is to us a “joint” Bayes classifier (using the joint probability of the features x, as opposed to a naive Bayes classifier).\n",
    "\n",
    "### Compute the exact number of parameters in naive Bayes classifier and the “joint” Bayes classifier.\n",
    "\n",
    "Naive Bayes Classifier has 5 features with 2 values, so there are 10 parameters.\n",
    "In the joint Bayes Classifier, it utilizes 2^5 - 1 parameters, 31.\n",
    "\n",
    "\n",
    "### Why should we probably not use the joint model for these data? (10 points)\n",
    "In the case of a joint Bayes Classifier, there will be some cases wherein the feature parameter x=0 would cause the Bayes classifier to predict 0 probability in the validation data.  This would be problematic, because misclassification could result, because the problem statement mandates that in the case of ties, we prefer to predict +1.  This is an example of increased unwanted complexity that could ultimately lead to overfitting the model.\n",
    "\n",
    "### Suppose that, before we make our predictions, we lose access to my address book, so that we cannot tell whether the email author is known. Should we re-train the model, and if so, how? (e.g.: how does the model, and its parameters, change in this new situation?). Hint: what will the naive Bayes model over only features x2...x5 look like, and what will its parameters be?\n",
    "$P(y=1 | X = x_2, x_3, x_4, x_5) = $\n",
    "\n",
    "$\\frac{P(X = x_2, x_3, x_4, x_5 | y =1) P(y=1)} {P(X = x_2, x_3, x_4, x_5 | y =1) P(y=1) + P(X = x_2, x_3, x_4, x_5 | y =-1) P(y=-1)}$\n",
    "\n",
    "Naive Bayes assumes that the features are independent of each other.\n",
    "So, if there are only features $x2,.., x5$, the parameters of the naive Bayes model would all be the same, except we would not be able to compute $P(x_1 | y=1)$ and $P(x_1 | y=-1)$.\n",
    "So, retraining the model would not be necessary, we would just carry on as delineated in the formula provided above, without the use of the $x_1$ parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}